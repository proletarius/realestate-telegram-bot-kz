import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urlencode
import re
import unidecode

BASE_URL = "https://krisha.kz"

CITY_MAP = {
    "–∞—Å—Ç–∞–Ω–∞": "astana",
    "–∞–ª–º–∞—Ç—ã": "almaty",
    "—à—ã–º–∫–µ–Ω—Ç": "shymkent",
    "–∫–∞—Ä–∞–≥–∞–Ω–¥–∞": "karaganda",
    "–∞–∫—Ç–æ–±–µ": "aktobe",
    "–∞—Ç—ã—Ä–∞—É": "atyrau",
    "—É—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫": "oskemen",
    "–ø–∞–≤–ª–æ–¥–∞—Ä": "pavlodar",
    "—Ç–∞—Ä–∞–∑": "taraz",
    "–∫–æ—Å—Ç–∞–Ω–∞–π": "kostanay"
    # –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
}

def normalize_city(city: str) -> str:
    city = city.strip().lower()
    if city in CITY_MAP:
        return CITY_MAP[city]

    # –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã ‚Üí –ª–∞—Ç–∏–Ω–∏—Ü–∞
    latin_city = unidecode.unidecode(city)
    # –£–¥–∞–ª–∏–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    latin_city = re.sub(r"[^a-z0-9-]", "", latin_city)
    return latin_city or "astana"

async def fetch_html(url):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0"
        ),
        "Accept-Language": "ru-RU,ru;q=0.9",
        "Accept": "text/html,application/xhtml+xml"
    }
    async with aiohttp.ClientSession(headers=headers) as session:
        async with session.get(url) as resp:
            return await resp.text()


def build_search_url(city, operation_type, property_type, rooms, max_price):
    operation_map = {
        "–∞—Ä–µ–Ω–¥–∞": "arenda",
        "–ø–æ–∫—É–ø–∫–∞": "prodazha"
    }

    property_map = {
            "–∫–≤–∞—Ä—Ç–∏—Ä–∞": "kvartiry",
            "–¥–æ–º": "doma"
        }

    op = operation_map.get(operation_type.lower(), "arenda")
    prop = property_map.get(property_type.lower(), "kvartiry")
    city_slug = normalize_city(city)


    query = {
        "das[rooms]": rooms,
        "das[price][to]": max_price
    }
 
    return f"{BASE_URL}/{op}/{prop}/{city_slug}/?{urlencode(query)}"


async def parse_krisha(city, operation_type, property_type, rooms, max_price):
    url = build_search_url(city, operation_type, property_type, rooms, max_price)
    print("üîó URL:", url)

    html = await fetch_html(url)
    soup = BeautifulSoup(html, "html.parser")

    cards = soup.select("div.a-card")
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫: {len(cards)}")

    items = []
    for card in cards:
        try:
            title_el = card.select_one("a.a-card__title")
            price_el = card.select_one("div.a-card__price")
            address_el = card.select_one("div.a-card__subtitle")
            desc_el = card.select_one("div.a-card__text-preview")

            if not title_el or not price_el:
                continue

            title = title_el.get_text(strip=True)
            price = price_el.get_text(strip=True)
            address = address_el.get_text(strip=True) if address_el else ""
            description = desc_el.get_text(strip=True) if desc_el else ""
            relative_url = title_el.get("href", "")
            full_url = BASE_URL + relative_url if relative_url.startswith("/") else relative_url

            items.append({
                "title": title,
                "price": price,
                "url": full_url,
                "address": address,
                "description": description
            })
        except Exception as e:
            print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è:", e)
            continue

    return items

"""
if __name__ == "__main__":
    import asyncio
    results = asyncio.run(parse_krisha(
        city="–ê–ª–º–∞—Ç—ã",
        operation_type="–ø–æ–∫—É–ø–∫–∞",
        property_type="–∫–≤–∞—Ä—Ç–∏—Ä–∞",
        rooms=3,
        max_price=1500000
    ))

    for r in results:
        print(f"{r['price']} ‚Äî {r['title']}")
        print(f"{r['address']}")
        print(f"{r['url']}")
        print(f"{r['description']}\n")
"""