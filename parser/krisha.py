import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urlencode
import re
import unidecode
import logging
from bot.utils.logging import setup_logging  # –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–æ–≤ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
setup_logging()
logger = logging.getLogger(__name__)

BASE_URL = "https://krisha.kz"

CITY_MAP = {
    "–∞—Å—Ç–∞–Ω–∞": "astana",
    "–∞–ª–º–∞—Ç—ã": "almaty",
    "—à—ã–º–∫–µ–Ω—Ç": "shymkent",
    "–∫–∞—Ä–∞–≥–∞–Ω–¥–∞": "karaganda",
    "–∞–∫—Ç–æ–±–µ": "aktobe",
    "–∞—Ç—ã—Ä–∞—É": "atyrau",
    "—É—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫": "oskemen",
    "–ø–∞–≤–ª–æ–¥–∞—Ä": "pavlodar",
    "—Ç–∞—Ä–∞–∑": "taraz",
    "–∫–æ—Å—Ç–∞–Ω–∞–π": "kostanay"
    # –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
}

def normalize_city(city: str) -> str:
    city = city.strip().lower()
    if city in CITY_MAP:
        return CITY_MAP[city]

    latin_city = unidecode.unidecode(city)
    latin_city = re.sub(r"[^a-z0-9-]", "", latin_city)
    return latin_city or "astana"

async def fetch_html(url):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0"
        ),
        "Accept-Language": "ru-RU,ru;q=0.9",
        "Accept": "text/html,application/xhtml+xml"
    }
    async with aiohttp.ClientSession(headers=headers) as session:
        async with session.get(url) as resp:
            return await resp.text()

def build_search_url(city, operation_type, property_type, rooms, max_price, search_text=None, year_built=None, land_type=None):
    operation_map = {
        "–∞—Ä–µ–Ω–¥–∞": "arenda",
        "–ø–æ–∫—É–ø–∫–∞": "prodazha"
    }

    city_slug = normalize_city(city)
    op = operation_map.get(operation_type.lower(), "arenda")
    prop = "doma-dachi" if property_type == "–¥–æ–º" else "kvartiry"

    query = {}

    if max_price is not None:
        query["das[price][to]"] = max_price

    if rooms is not None:
        if property_type == "–¥–æ–º":
            query["das[live.rooms]"] = rooms
        else:
            query["das[rooms]"] = rooms

    if year_built and property_type == "–¥–æ–º":
        query["das[house.year][from]"] = year_built

    txt_parts = []
    if land_type == "–ò–ñ–°" and property_type == "–¥–æ–º":
        txt_parts.append("–ò–ñ–°")

    if search_text:
        txt_parts.extend(search_text.strip().split())

    manual_keywords = txt_parts[1:] if txt_parts else []

    if txt_parts:
        query["_txt_"] = txt_parts[0]  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ ‚Äî –¥–ª—è Krisha

    url = f"{BASE_URL}/{op}/{prop}/{city_slug}/?{urlencode(query)}"
    return url, manual_keywords

async def parse_krisha(city, operation_type, property_type, rooms, max_price, search_text=None, year_built=None, land_type=None):
    url, manual_keywords = build_search_url(city, operation_type, property_type, rooms, max_price, search_text, year_built, land_type)
    logger.info("üîó Krisha URL: %s", url)

    html = await fetch_html(url)
    if not html:
        logger.error("‚ùå –ü—É—Å—Ç–æ–π HTML —Å URL: %s", url)
        return []

    soup = BeautifulSoup(html, "html.parser")
    cards = soup.select("div.a-card")

    if not cards:
        logger.error("‚ùå Krisha –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –Ω–∏ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏! –í–æ–∑–º–æ–∂–Ω–æ, —Å–∞–π—Ç –∏–∑–º–µ–Ω–∏–ª—Å—è –∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä —Å–ª–∏—à–∫–æ–º —É–∑–∫–∏–π.")
    else:
        logger.info("üîç –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫: %d", len(cards))

    items = []
    for card in cards:
        try:
            title_el = card.select_one("a.a-card__title")
            price_el = card.select_one("div.a-card__price")
            address_el = card.select_one("div.a-card__subtitle")
            desc_el = card.select_one("div.a-card__text-preview")

            if not title_el or not price_el:
                continue

            title = title_el.get_text(strip=True)
            price = price_el.get_text(strip=True)
            address = address_el.get_text(strip=True) if address_el else ""
            description = desc_el.get_text(strip=True) if desc_el else ""
            relative_url = title_el.get("href", "")
            full_url = BASE_URL + relative_url if relative_url.startswith("/") else relative_url

            full_text = f"{title.lower()} {description.lower()}"

            if manual_keywords and not all(kw.lower() in full_text for kw in manual_keywords):
                continue

            items.append({
                "title": title,
                "price": price,
                "url": full_url,
                "address": address,
                "description": description
            })

        except Exception as e:
            logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–∞—Ä—Ç–æ—á–∫–∏: %s", e)
            continue

    logger.info("‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ %d –∫–∞—Ä—Ç–æ—á–µ–∫ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É", len(items))
    return items
